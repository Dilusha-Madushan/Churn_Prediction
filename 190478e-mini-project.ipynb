{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"# Import Libraries\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Import Datasets\"\"\"\ndf_train = pd.read_csv('/kaggle/input/cs-3110-mini-project/train.csv')\ndf_test = pd.read_csv('/kaggle/input/cs-3110-mini-project/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"# Define Functions\"\"\"\n\ndef set_outliers_to_nan(p , feature_boundries):\n  df = p.copy()\n\n  for key in feature_boundries.keys():\n    feature = key\n    lower_bound = feature_boundries[key][0]\n    upper_bound = feature_boundries[key][1]\n    print(feature , lower_bound , upper_bound)\n    df.loc[df[feature] < lower_bound , feature] = np.nan\n    df.loc[df[feature] > upper_bound, feature] = np.nan\n  return df\n\ndef isNaN(num):\n    if float('-inf') < float(num) < float('inf'):\n        return False\n    else:\n        return True\n\ndef linear_regression_imputation_to_nan(p, corr_features):\n    df = p.copy()\n    for c_f in corr_features:\n        # impute median for both same index values are nan\n        f1 = c_f[0]\n        f2 = c_f[1]\n        df.loc[df[f1].isnull() & df[f2].isnull(), f1] = df[f1].median()\n        df.loc[df[f1].isnull() & df[f2].isnull(), f2] = df[f2].median()\n\n        model1 = LinearRegression()\n        model2 = LinearRegression()\n        data = df.dropna(subset=c_f)\n        \n        data_f1 = data[[f1]]\n        data_f2 = data[[f2]]\n      \n        model1.fit(data_f1, data_f2)\n        model2.fit(data_f2, data_f1)\n        \n        for i in df.index:\n            if isNaN(df[f1][i]):\n                x = np.array([[df[f2][i]]])\n                df[f1][i] = model2.predict(x)[0][0]\n\n            elif isNaN(df[f2][i]):\n                x = np.array([[df[f1][i]]])\n\n                df[f2][i] = model1.predict(x)[0][0]\n\n    return df\n\ndef categorical_imputation_by_most_frequent_value(p, features):\n    df = p.copy()\n    for f in features:\n        frq_val = df_train[~df_train[f].isnull()][f].value_counts().idxmax()\n        df[f] = df[f].fillna(frq_val)\n    return df\n\ndef nan_imputation_by_median(p, features):\n    df = p.copy()\n    for feature in features:\n        df.loc[df[feature].isnull(), feature] = df[feature].median()\n    return df\n\ndef show_correlations(dataframe, show_chart = True):\n    fig = plt.figure(figsize = (20,10))\n    corr = dataframe.corr()\n    if show_chart == True:\n        sns.heatmap(corr, \n                    xticklabels=corr.columns.values,\n                    yticklabels=corr.columns.values,\n                    annot=True)\n    return corr\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Define Variables\"\"\"\n\ncat_col = ['location_code' , 'intertiol_plan' , 'voice_mail_plan' , 'Churn']\n\nnum_col = ['account_length',\n             'number_vm_messages',\n             'total_day_min',\n             'total_day_calls',\n             'total_day_charge',\n             'total_eve_min',\n             'total_eve_calls',\n             'total_eve_charge',\n             'total_night_minutes',\n             'total_night_calls',\n             'total_night_charge',\n             'total_intl_minutes',\n             'total_intl_calls',\n             'total_intl_charge',\n             'customer_service_calls']\n\nfeature_boundries = {'account_length': [0, 210],\n                    'number_vm_messages': [0, 50],\n                    'total_day_min': [0, 350],\n                    'total_day_calls': [0, 160],\n                    'total_day_charge': [0, 60],\n                    'total_eve_min': [0, 400],\n                    'total_eve_calls': [40, 170],\n                    'total_eve_charge': [3, 31],\n                    'total_night_minutes': [23, 400],\n                    'total_night_calls': [30, 175],\n                    'total_night_charge': [0, 20],\n                    'total_intl_minutes': [0, 20],\n                    'total_intl_calls': [0, 18],\n                    'total_intl_charge': [0, 5],\n                    'customer_service_calls': [0, 9]}\n\ncorrelated_feature_couple = [['total_eve_charge', 'total_eve_min'],\n                   ['total_night_charge', 'total_night_minutes'],\n                   ['total_intl_charge', 'total_intl_minutes'],\n                   ['total_day_charge', 'total_day_min']]\n\ncat_col1 = ['location_code' , 'intertiol_plan' , 'voice_mail_plan']\n\nnon_corr_numer_col = ['account_length',\n             'number_vm_messages',\n             'total_day_calls',\n             'total_eve_calls',\n             'total_night_calls',\n             'total_intl_calls',\n             'customer_service_calls']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Drop Unwanted Columns\"\"\"\n\ndf_train = df_train.drop(columns=['Unnamed: 20'] , errors='ignore')\n\ndf_test = df_test.drop(columns=['Unnamed: 20' , 'Unnamed: 19'] , errors='ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Drop Duplicates\"\"\"\n\ndf1 = df_train.drop(columns=['customer_id'])\n\nduplicates = df1.duplicated()\n\ni = 0\ndup_index = []\nwhile i<=2320:\n  if(duplicates[i]): dup_index.append(i)\n  i += 1\n\ndf_train = df_train.drop(labels=dup_index, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Handle Invalid Data\"\"\"\n\nfor i in num_col:\n  df_train[i] = np.where(df_train[i] < 0, np.NaN , df_train[i])\n\nfor i in num_col:\n  df_test[i] = np.where(df_test[i] < 0, np.NaN , df_test[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Handle Outliers\"\"\"\n\ndf_train = set_outliers_to_nan(df_train , feature_boundries)\n\ndf_test = set_outliers_to_nan(df_test , feature_boundries)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Handle Missing Values\"\"\"\n\ndf_train = linear_regression_imputation_to_nan(df_train , correlated_feature_couple)\n\ndf_train = categorical_imputation_by_most_frequent_value(df_train , cat_col1)\n\ndf_train = nan_imputation_by_median(df_train , non_corr_numer_col)\n\ndf_train = df_train[~df_train['Churn'].isnull()]\n\ndf_test = linear_regression_imputation_to_nan(df_test , correlated_feature_couple)\n\ndf_test = categorical_imputation_by_most_frequent_value(df_test , cat_col1)\n\ndf_test = nan_imputation_by_median(df_test , non_corr_numer_col)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_onehot = df_train.copy()\ndf_test_onehot = df_test.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc = OneHotEncoder(handle_unknown='ignore')\nenc_df_train = pd.DataFrame(enc.fit_transform(df_train_onehot[['location_code']]).toarray())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc_df_test = pd.DataFrame(enc.fit_transform(df_test_onehot[['location_code']]).toarray())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onehot_col = ['location_code_452' , 'location_code_445' ,'location_code_547']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_onehot.insert(2 , 'location_code_452' , enc_df_train[0].values , False)\ndf_train_onehot.insert(3 , 'location_code_445' , enc_df_train[1].values , False)\ndf_train_onehot.insert(4 , 'location_code_547' , enc_df_train[2].values , False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_onehot.insert(2 , 'location_code_452' , enc_df_test[0].values , False)\ndf_test_onehot.insert(3 , 'location_code_445' , enc_df_test[1].values , False)\ndf_test_onehot.insert(4 , 'location_code_547' , enc_df_test[2].values , False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[onehot_col] = df_train_onehot[onehot_col]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[onehot_col] = df_test_onehot[onehot_col]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=['location_code'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(columns=['location_code'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.voice_mail_plan = df_train.voice_mail_plan.map(dict(yes=1, no=0))\ndf_train.intertiol_plan = df_train.intertiol_plan.map(dict(yes=1, no=0))\ndf_train.Churn = df_train.Churn.map(dict(Yes=1, No=0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.voice_mail_plan = df_test.voice_mail_plan.map(dict(yes=1, no=0))\ndf_test.intertiol_plan = df_test.intertiol_plan.map(dict(yes=1, no=0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = list(df_train.columns)\ncol.remove('customer_id')\ncol.remove('Churn')\ncol","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_df = show_correlations(df_train[col],show_chart=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=1)\n\n# pca.fit(df_train[['number_vm_messages' , 'voice_mail_plan']])\n# X_pca5 = pca.transform(df_train[['number_vm_messages' , 'voice_mail_plan']])\n# df_5p = pd.DataFrame(data = X_pca5 , columns=['PCA1'] , index=df_train.index)\n\n# pca.fit(df_test[['number_vm_messages' , 'voice_mail_plan']])\n# X_pca55 = pca.transform(df_test[['number_vm_messages' , 'voice_mail_plan']])\n# df_55p = pd.DataFrame(data = X_pca55 , columns=['PCA1'] , index=df_test.index)\n\n# df_train['PC1'] = df_5p['PCA1']\n# df_test['PC1'] = df_55p['PCA1']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['total_intl_charge_per_min'] = df_train['total_intl_charge'] / df_train['total_intl_minutes']\n# df_test['total_intl_charge_per_min'] = df_test['total_intl_charge'] / df_test['total_intl_minutes']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['total_night_charge_per_min']= df_train['total_night_charge'] / df_train['total_night_minutes']\n# df_test['total_night_charge_per_min'] = df_test['total_night_charge'] / df_test['total_night_minutes']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['total_eve_charge_per_min'] = df_train['total_eve_charge'] / df_train['total_eve_min']\n# df_test['total_eve_charge_per_min'] = df_test['total_eve_charge'] / df_test['total_eve_min']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['total_day_charge_per_min'] = df_train['total_day_charge'] / df_train['total_day_min']\n# df_test['total_day_charge_per_min'] = df_test['total_day_charge'] / df_test['total_day_min']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['total_intl_charge_per_min'] = df_train['total_intl_charge_per_min'].fillna(0)\n# df_test['total_intl_charge_per_min'] = df_test['total_intl_charge_per_min'].fillna(0)\n\n# df_train['total_night_charge_per_min'] = df_train['total_night_charge_per_min'].fillna(0)\n# df_test['total_night_charge_per_min'] = df_test['total_night_charge_per_min'].fillna(0)\n\n# df_train['total_eve_charge_per_min']  = df_train['total_eve_charge_per_min'].fillna(0)\n# df_test['total_eve_charge_per_min'] = df_test['total_eve_charge_per_min'].fillna(0)\n\n# df_train['total_day_charge_per_min'] = df_train['total_day_charge_per_min'].fillna(0)\n# df_test['total_day_charge_per_min'] = df_test['total_day_charge_per_min'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = df_train.drop(columns=['total_intl_charge' , 'total_intl_minutes' , 'total_night_charge' , 'total_night_minutes' ,'total_eve_charge' , 'total_eve_min' , 'total_day_charge' , 'total_day_min' , 'number_vm_messages'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = df_test.drop(columns=['total_intl_charge' , 'total_intl_minutes' , 'total_night_charge' , 'total_night_minutes' ,'total_eve_charge' , 'total_eve_min' , 'total_day_charge' , 'total_day_min' , 'number_vm_messages'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['total_charge'] = df_train['total_intl_charge'] + df_train['total_night_charge'] + df_train[\n    'total_eve_charge'] + df_train['total_day_charge']\n\ndf_train['total_calls'] = df_train['total_intl_calls'] + df_train['total_night_calls'] + df_train['total_eve_calls'] + \\\n                          df_train['total_day_calls']\n\ndf_train['total_min'] = df_train['total_intl_minutes'] + df_train['total_night_minutes'] + df_train['total_eve_min'] + \\\n                        df_train['total_day_min']\n\ndf_train[\"no_of_plans\"] = df_train['intertiol_plan'] + df_train['voice_mail_plan']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['total_charge'] = df_test['total_intl_charge'] + df_test['total_night_charge'] + df_test[\n    'total_eve_charge'] + df_test['total_day_charge']\n\ndf_test['total_calls'] = df_test['total_intl_calls'] + df_test['total_night_calls'] + df_test['total_eve_calls'] + \\\n                          df_test['total_day_calls']\n\ndf_test['total_min'] = df_test['total_intl_minutes'] + df_test['total_night_minutes'] + df_test['total_eve_min'] + \\\n                        df_test['total_day_min']\n\ndf_test[\"no_of_plans\"] = df_test['intertiol_plan'] + df_test['voice_mail_plan']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=['customer_id'])\ndf_test = df_test.drop(columns=['customer_id'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_df = show_correlations(df_train,show_chart=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = df_train.drop(columns=['total_day_min' ,'number_vm_messages' , 'total_eve_min' ,  'total_night_minutes' , 'total_intl_minutes'] , errors='ignore')\n# df_test = df_test.drop(columns=['total_day_min' ,'number_vm_messages' , 'total_eve_min' ,  'total_night_minutes' , 'total_intl_minutes'] , errors='ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y = df_train['Churn']\n# X = df_train.drop(columns = ['Churn'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_over = df_train.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_class_0, count_class_1 = df_over.Churn.value_counts()\n\n# Divide by class\ndf_class_0 = df_over[df_over['Churn'] == 0]\ndf_class_1 = df_over[df_over['Churn'] == 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_train_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_train_over.Churn.value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train_over.drop('Churn',axis='columns')\ny = df_train_over['Churn']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = np.nan_to_num(X_train.astype(np.float32))\nX_test_new = np.nan_to_num(X_test.astype(np.float32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Random Forest model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,\n                                  random_state =50, max_features = \"auto\",\n                                  max_leaf_nodes = 30)\nX_train_new = np.nan_to_num(X_train.astype(np.float32))\nX_test_new = np.nan_to_num(X_test.astype(np.float32))\n\nmodel_rf.fit(X_train_new, y_train)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test_new)\nprint (metrics.accuracy_score(y_test, prediction_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cm = confusion_matrix(y_test, prediction_test)\n# plt.figure(figsize=(7,5))\n# sns.heatmap(cm, annot=True)\n# plt.xlabel('Predicted')\n# plt.ylabel('Truth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## XG Boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel3 = XGBClassifier()\nmodel3.fit(X_train_new, y_train)\npreds = model3.predict(X_test_new)\nmetrics.accuracy_score(y_test, preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CatBoost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n\nmodel4 = CatBoostClassifier(\n    iterations=1000, \n    learning_rate=0.05, \n    #loss_function='CrossEntropy'\n)\n\n\nmodel4.fit(X_train_new, y_train)\n\npreds = model4.predict(X_test_new)\n#metrics.accuracy_score(y_test, preds)\n\nmean_squared_error(y_test, preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test, preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_names = ['True Neg','False Pos','False Neg','True Pos']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.asarray(labels).reshape(2,2)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## CatBoost is selected as final model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new = np.nan_to_num(X.astype(np.float32))\nX_test = np.nan_to_num(df_test.astype(np.float32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submission(y_preds , name):\n    df_sub = pd.read_csv('/kaggle/input/cs-3110-mini-project/test.csv')[['customer_id']]\n    df_sub['Churn'] = y_preds\n\n    df_sub[df_sub['Churn']==1] = 'Yes'\n    df_sub[df_sub['Churn']==0] = 'No'\n\n    df_sub['customer_id'] = pd.read_csv('/kaggle/input/cs-3110-mini-project/test.csv')[['customer_id']]['customer_id']\n\n    file_name = \"sample_submission\" + name + \".csv\"\n\n    df_sub.to_csv(file_name , index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n\nmodel4 = CatBoostClassifier(\n    iterations=2500, \n    learning_rate=0.05, \n    depth=9\n    #loss_function='CrossEntropy'\n)\n\n\nmodel4.fit(X_new, y)\n\npreds = model4.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.read_csv('/kaggle/input/cs-3110-mini-project/test.csv')[['customer_id']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['Churn'] = preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[df_sub['Churn']==1] = 'Yes'\ndf_sub[df_sub['Churn']==0] = 'No'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['customer_id'] = pd.read_csv('/kaggle/input/cs-3110-mini-project/test.csv')[['customer_id']]['customer_id']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv('sample_submission4.csv' , index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params = {\n     'catboost': {\n        'model': CatBoostClassifier(),\n        'params' : {\n            'depth':[8 , 9 , 10],\n            'iterations':[2000 , 3000 , 4000],\n            'learning_rate':[0.01 , 0.05 , 0.1]\n        }\n    }\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = []\n\nfor model_name, mp in model_params.items():\n    print(model_name)\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=3, return_train_score=False)\n    clf.fit(X_new,y)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    print('done ' , model_name)\n    \ndf_tun = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf_tun","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop('Churn',axis='columns')\ny = df_train['Churn']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_over = GradientBoostingClassifier(n_estimators=400, learning_rate=0.01,max_depth=13)\n\nclf_over.fit(X, y)\n\ny_preds = clf_over.predict(df_test)\nsubmission(y_preds , '_over3')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scale_train = df_train.copy()\ndf_scale_test = df_test.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature Scaling  \nfrom sklearn.preprocessing import StandardScaler ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st_x= StandardScaler()   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scale_train= st_x.fit_transform(X)    \ndf_scale_test= st_x.transform(df_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_over = GradientBoostingClassifier(n_estimators=400, learning_rate=0.01,max_depth=13)\n\nclf_over.fit(df_scale_train, y)\n\ny_preds = clf_over.predict(df_scale_test)\nsubmission(y_preds , '_scale1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n\nmodel4 = CatBoostClassifier(\n    iterations=1500, \n    learning_rate=0.01, \n    depth=13\n    #loss_function='CrossEntropy'\n)\n\n\nmodel4.fit(df_scale_train, y)\n\npreds = model4.predict(df_scale_test)\nsubmission(preds , '_scale6')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier  \nclassifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \nclassifier.fit(X_train, y_train)\n\n#Predicting the test set result  \ny_preds= classifier.predict(X_test)\n#submission(preds , '_scale3')\nmean_squared_error(y_test, y_preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC # \"Support vector classifier\"  \nclassifier = SVC(kernel='linear', random_state=0)  \nclassifier.fit(X_train, y_train)\n\ny_preds= classifier.predict(X_test)\n#submission(preds , '_scale4')\nmean_squared_error(y_test, y_preds)","metadata":{},"execution_count":null,"outputs":[]}]}